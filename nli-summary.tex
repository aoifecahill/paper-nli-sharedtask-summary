%
% File naaclhlt2012.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{linguex}

\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{A Report on the First Native Language Identification Shared Task}

\author{Joel Tetreault$^{*}$, Daniel Blanchard$^{\dag}$ and Aoife Cahill$^{\dag}$\\
\\
  { $^{*}$ Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA}\\
  { {\tt Joel.Tetreault@nuance.com}} \\
  { $^{\dag}$ Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA}\\
  { {\tt \{dblanchard, acahill\}@ets.org}}\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\section{Introduction}
\label{sec-intro}
\newcite{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR}
 / Motivation for NLI paper (I figure we can take this from the CoNLL proposal I wrote up last year)

One quickly growing subfield in NLP is the task of identifying the native
language (L1) of a writer based solely on a sample of their writing. The task
is framed as a classification problem where the set of L1s is known a priori.
Most work has focused on identifying the native language of writers
learning English as a second language. To date this topic has motivated
several ACL and EMNLP papers, as well as a master's thesis \cite{kochmar2011identification}.

Native Language Identification (NLI) can be useful for a number of
applications. NLI can be used in educational settings to provide more
targeted feedback to language learners about their errors. It is
well known that speakers of different languages make different kinds of
errors when learning a language \cite{SwanSmith01}. A writing tutor system
which can detect the native language of the learner will be able to tailor
the feedback about the error and contrast it with common
properties of the learner's language. In addition, native language is
often used as a feature that goes into authorship profiling
\cite{estival2007author}, which is frequently used in forensic linguistics.

Despite the growing interest in this field, development has been encumbered
by two issues. First is the issue of data. Evaluating an NLI system requires
 a corpus containing texts in a language other than the native language of
the writer. Because of a scarcity of such corpora, most work has used
the International Corpus of Learner English (ICLEv2) \cite{Granger2009}
for training and evaluation since it contains several hundred essays
written by college-level English language learners. However,
this corpus is quite small for training and testing statistical systems
which makes it difficult to tell whether the systems that are developed can
scale well to larger data sets or to different domains.

The usability of the corpus is further compromised by idiosyncrasies in the
data such as topic bias (as shown by \cite{brooke2011native}) and the
occurrence of characters which only appear in essays
written by speakers of certain languages. As a result, it is hard to draw
conclusions about which features actually perform best. The second issue is
that there has been little consistency in the field in the use of
cross-validation, the number of L1s, and which L1s are used. As a result,
comparing one approach to another has been extremely difficult.

The goal of this proposal is to create the first shared task in Native
Language Identification (NLI) to better unify this community and help the
field progress. The Shared Task proposal addresses the two deficiencies above
by first using a new corpus (TOEFL11, discussed below) which is larger than
the ICLE and designed specifically for the task of NLI and second, by
providing a common set of L1s and evaluation standards that everyone will use
for this competition, thus facilitating direct comparison of approaches. This
task is very appropriate for CoNLL since most previous approaches
to NLI have used different machine learning and statistical techniques
ranging from classifiers using lexical and n-gram features
\cite{koppel2005determining}, to approaches using Tree Substitution Grammars
\cite{swanson-charniak:2012:ACL2012short}, to those which make use of
topic models \cite{Sze-MengJojoWongMarkDrasMarkJohnson:2011:ALTA2011}.


\section{Related Work}

(briefly mention the methods that people have used)



\section{Data}
The dataset for the task was the new TOEFL11 corpus
\cite{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR}. TOEFL11
consists of essays written during a high-stakes college-entrance test, the Test
of English as a Foreign Language (TOEFL\textsuperscript{\textregistered}). The
corpus contains 1,100 essays per language sampled as evenly as possible from 8
prompts (i.e., topics) along with score levels (low/medium/high) for each essay.
The 11 native languages covered by our corpus are: Arabic (ARA), Chinese
(CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese
(JAP), Korean (KOR), Spanish (SPA), Telugu (TEL), and Turkish (TUR).

The TOEFL11 corpus was designed specifically to support the task of native
language identification.  Because all of the essays were collected through ETS'
operational test delivery system for the TOEFL\textsuperscript{\textregistered}
test, the encoding and storage of all texts in the corpus is consistent.
Furthermore, the sampling of essays was designed to ensure approximately equal
representation of native languages across topics, insofar as this was possible.

For the shared task, the corpus was split into three sets: training (TOEFL11-TRAIN),
development (TOEFL11-DEV), and test (TOEFL11-TEST). The train corpus consisted
of 900 essays per L1, the development set consisted of 100 essays per L1,
and the test set consisted of another 100 essays per L1.  
Although the overall TOEFL11 corpus was sampled as evenly
as possible with regard to language and prompts, the distribution for each
language is not exactly the same in the training, development and test sets (see
Tables~\ref{tab:prompts-train}, \ref{tab:prompts-dev}, and
\ref{tab:prompts-test}). In fact, the distribution is much closer between the
training and test sets, as there are several languages for which there are no
essays for a given prompt in the development set, whereas there are none in the
training set, and only one, Italian, for the test set.

It should be noted that in the first instantiation of the corpus,
presented in \cite{tetreault-EtAl:2012:PAPERS},

\begin{table*}[htb]
    \label{tab:prompts-train}
    \begin{center}
        \begin{tabular}{lrrrrrrrr}
            \hline
            \textbf{Lang.} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8}\\ \hline
            Ara.     &  113 & 113 & 113 & 112 & 112 & 113 & 112 & 112 \\ \hline
            Chi.    &  113 & 113 & 113 & 112 & 112 & 113 & 112 & 112 \\ \hline
            Fre.     &  128 & 128 & 76  & 127 & 127 & 60  & 127 & 127 \\ \hline
            Ger.     &  125 & 125 & 125 & 125 & 125 & 26  & 125 & 124 \\ \hline
            Hin.      &  132 & 132 & 132 & 71  & 132 & 38  & 132 & 131 \\ \hline
            Ita.    &  142 & 70  & 122 & 141 & 141 & 12  & 141 & 131 \\ \hline
            Jap.   &  108 & 114 & 113 & 113 & 113 & 113 & 113 & 113 \\ \hline
            Kor.     &  113 & 113 & 113 & 112 & 112 & 113 & 112 & 112 \\ \hline
            Spa.    &  124 & 120 & 38  & 124 & 123 & 124 & 124 & 123 \\ \hline
            Tel.     &  139 & 139 & 139 & 41  & 139 & 26  & 139 & 138 \\ \hline
            Tur.    &  132 & 132 & 72  & 132 & 132 & 37  & 132 & 131 \\ \hline \hline
            Total      &  1369 & 1299 & 1156 & 1210 & 1368 & 775 & 1369 & 1354 \\ \hline
        \end{tabular}
    \end{center}
    \caption{Number of essays per language per prompt in training set}
\end{table*}

\begin{table*}[htb]
    \label{tab:prompts-dev}
    \begin{center}
        \begin{tabular}{lrrrrrrrr}
            \hline
            \textbf{Lang.} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8}\\ \hline
            Ara.     &   12 & 13 & 13 & 13 & 14 & 7  & 14 & 14  \\ \hline
            Chi.    &   14 & 14 & 0  & 15 & 15 & 14 & 13 & 15  \\ \hline
            Fre.     &   17 & 18 & 0  & 14 & 19 & 0  & 13 & 19  \\ \hline
            Ger.     &   15 & 15 & 16 & 10 & 13 & 0  & 15 & 16  \\ \hline
            Hin.      &   16 & 17 & 17 & 0  & 17 & 0  & 16 & 17  \\ \hline
            Ita.    &   18 & 0  & 0  & 30 & 31 & 0  & 21 & 0   \\ \hline
            Jap.   &   0  & 14 & 15 & 14 & 15 & 14 & 14 & 14  \\ \hline
            Kor.     &   15 & 8  & 15 & 2  & 13 & 15 & 16 & 16  \\ \hline
            Spa.    &   7  & 0  & 0  & 21 & 7  & 21 & 21 & 23  \\ \hline
            Tel.     &   16 & 17 & 17 & 0  & 17 & 0  & 16 & 17  \\ \hline
            Tur.    &   22 & 4  & 0  & 22 & 7  & 0  & 22 & 23  \\ \hline \hline
            Total      &   152& 120& 93 & 141& 168& 71 & 181& 174 \\ \hline
        \end{tabular}
    \end{center}
    \caption{Number of essays per language per prompt in development set}
\end{table*}

\begin{table*}[htb]
    \label{tab:prompts-test}
    \begin{center}
        \begin{tabular}{lrrrrrrrr}
            \hline
            \textbf{Lang.} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8}\\ \hline
            Ara.     &   13 & 11 & 12 & 14 & 10 & 13 & 12 & 15 \\ \hline
            Chi.    &   13 & 14 & 13 & 13 & 7  & 14 & 14 & 12 \\ \hline
            Fre.     &   13 & 14 & 11 & 15 & 14 & 8  & 11 & 14 \\ \hline
            Ger.     &   15 & 14 & 16 & 16 & 12 & 2  & 12 & 13 \\ \hline
            Hin.      &   13 & 13 & 14 & 15 & 7  & 15 & 10 & 13 \\ \hline
            Ita.    &   13 & 19 & 16 & 16 & 15 & 0  & 11 & 10 \\ \hline
            Jap.   &   8  & 14 & 12 & 11 & 10 & 15 & 14 & 16 \\ \hline
            Kor.     &   12 & 12 & 8  & 14 & 12 & 14 & 13 & 15 \\ \hline
            Spa.    &   10 & 13 & 16 & 14 & 4  & 12 & 15 & 16 \\ \hline
            Tel.     &   10 & 10 & 11 & 14 & 13 & 15 & 11 & 16 \\ \hline
            Tur.    &   15 & 9  & 18 & 16 & 8  & 6  & 13 & 15 \\ \hline \hline
            Total      &   135&143 & 147& 158& 112& 114& 136& 155 \\ \hline
        \end{tabular}
    \end{center}
    \caption{Number of essays per language per prompt in test set}
\end{table*}


\section{NLI Shared Task Description}

The shared task consisted of three sub-tasks. For each task, the
test set was TOEFL11-TEST and only the type of training data varied from
task to task.  

\begin{itemize}
\item {\bf Closed-Training}:  The first and main task was the 11 way
classification task using only the TOEFL11-TRAIN and TRAIN-DEV for training.
\item {\bf Open-Training-1}: The second task allowed the use of any amount or
type of training data (as is done in \cite{brooke2011native}) excluding any data from the TOEFL11, but still evaluated on
TOEFL-TEST.
\item {\bf Open-Training-2}: The third task allowed the use of TOEFL11-TRAIN and TOEFL11-DEV combined with any other additional data.
\end{itemize}

Additionally, each team could submit up to 5 different
systems per task.  This allows a team to experiment with different
variations of their core system.  

The training data was released on January 14, with the development
data and evaluation script released nearly a month later on February 12. 
The train and dev data contained an index file with the L1 for each
essay in those sets.  The test data was released on March 11 and teams had 
8 days (March 19) to submit their system predictions.  The predictions for 
each system
are encoded in a csv file, where each line contains the file ID of a 
file in TOEFL-TEST and the corresponding L1 prediction made by the 
system.  Each csv file was emailed to the NLI organizers and then
evaluated against the gold standard.  Since this was a ``blind''
competition, the L1s for each of the essays in the test set were not
released until after the submission deadline.  


\section{Teams}

We had 29 teams compete in the shared task competition, with 24 teams
electing to write papers describing their system(s).  The list
of participating times, along with their team abbreviations, can be 
found in Table~\ref{tab:teams}.


\begin{table*}[htbp]
\begin{tabular}{|l|l|}
\hline
Team Name & Team Abbreviation \\ \hline
Bobicev & BOB  \\ \hline
Chonger & CHO  \\ \hline
CMU-Haifa & HAI  \\ \hline
Cologne-Nijmegen &  CN \\ \hline
CoRAL Lab @ UAB &  COR \\ \hline
CUNI (Charles University) &  CUN \\ \hline
cywu &  CYW \\ \hline
dartmouth & DAR  \\ \hline
eurac & EUR  \\ \hline
HAUTCS & HAU  \\ \hline
ItaliaNLP &  ITA \\ \hline
Jarvis &   JAR\\ \hline
kyle, crossley, dai, mcnamara & KYL  \\ \hline
LIMSI & LIM  \\ \hline
LTRC IIIT Hyderabad &  HYD \\ \hline
Michigan &  MIC \\ \hline
MITRE ``Carnie'' & CAR  \\ \hline
MQ &   MQ\\ \hline
NAIST &  NAI \\ \hline
NRC &  NRC \\ \hline
Oslo NLI & OSL  \\ \hline
Toronto &  TOR \\ \hline
Tuebingen &   TUE\\ \hline
Ualberta &  UAB \\ \hline
UKP &  UKP \\ \hline
Unibuc &  BUC \\ \hline
UNT &   UNT\\ \hline
UTD &  UTD \\ \hline
VTEX &  VTX \\ \hline

\end{tabular}
\caption{Participating Teams and Team Abbreviations\label{tab:teams}}
\end{table*}

\section{Shared Task Results}


\begin{table*}
\begin{small}
\begin{tabular}{|p{1cm}|c|p{1cm}|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline

\multicolumn{3}{|c|}{} & \multicolumn{11}{|c|}{\bf L1 F-Score}\\\hline
{\bf Team Name} & {\bf Run} & {\bf Overall Acc.} & {\bf ARA} & {\bf CHI} & {\bf FRE} & {\bf GER} & {\bf HIN} & {\bf ITA} & {\bf JPN} & {\bf KOR} & {\bf SPA} & {\bf TEL} & {\bf TUR}\\\hline
  JAR & 2  & 0.836 & 0.785 & 0.856 & 0.860 & 0.893 & 0.775 & 0.905 & 0.854 & 0.813 & 0.798 & 0.802 & 0.854\\\hline
OSL   & 2  & 0.834 & 0.816 & 0.850 & 0.874 & 0.912 & 0.792 & 0.873 & 0.828 & 0.806 & 0.783 & 0.792 & 0.840\\\hline
 BUC  & 5  & 0.827 & 0.840 & 0.866 & 0.853 & 0.931 & 0.736 & 0.873 & 0.851 & 0.812 & 0.779 & 0.760 & 0.796\\\hline
CAR   & 2  & 0.826 & 0.859 & 0.847 & 0.810 & 0.921 & 0.762 & 0.877 & 0.825 & 0.827 & 0.768 & 0.802 & 0.790\\\hline
  TUE & 1  & 0.822 & 0.810 & 0.853 & 0.806 & 0.897 & 0.768 & 0.883 & 0.842 & 0.776 & 0.772 & 0.824 & 0.812\\\hline
 NRC  & 4  & 0.818 & 0.804 & 0.845 & 0.848 & 0.916 & 0.745 & 0.903 & 0.818 & 0.790 & 0.788 & 0.755 & 0.790\\\hline
HAI   & 1  & 0.815 & 0.804 & 0.842 & 0.835 & 0.903 & 0.759 & 0.845 & 0.825 & 0.806 & 0.776 & 0.789 & 0.784\\\hline
 CN  & 2  & 0.814 & 0.778 & 0.845 & 0.848 & 0.882 & 0.744 & 0.857 & 0.812 & 0.779 & 0.787 & 0.784 & 0.827\\\hline
 NAI  & 1  & 0.811 & 0.814 & 0.829 & 0.828 & 0.876 & 0.755 & 0.864 & 0.806 & 0.789 & 0.757 & 0.793 & 0.802\\\hline
 UTD  & 2  & 0.809 & 0.778 & 0.846 & 0.832 & 0.892 & 0.731 & 0.866 & 0.846 & 0.819 & 0.715 & 0.784 & 0.784\\\hline
 UAB  & 3  & 0.803 & 0.820 & 0.804 & 0.822 & 0.905 & 0.724 & 0.850 & 0.811 & 0.736 & 0.777 & 0.792 & 0.786\\\hline
 TOR  & 1  & 0.802 & 0.754 & 0.827 & 0.827 & 0.878 & 0.722 & 0.850 & 0.820 & 0.808 & 0.747 & 0.784 & 0.798\\\hline
  MQ & 4  & 0.801 & 0.800 & 0.828 & 0.789 & 0.885 & 0.738 & 0.863 & 0.826 & 0.780 & 0.703 & 0.782 & 0.802\\\hline
 CYW  & 1  & 0.797 & 0.769 & 0.839 & 0.782 & 0.833 & 0.755 & 0.842 & 0.815 & 0.770 & 0.741 & 0.828 & 0.788\\\hline
DAR   & 2  & 0.781 & 0.761 & 0.806 & 0.812 & 0.870 & 0.706 & 0.846 & 0.788 & 0.776 & 0.730 & 0.723 & 0.767\\\hline
 ITA  & 1  & 0.779 & 0.738 & 0.775 & 0.832 & 0.873 & 0.711 & 0.860 & 0.788 & 0.742 & 0.708 & 0.762 & 0.780\\\hline
CHO  & 1  & 0.775 & 0.764 & 0.835 & 0.798 & 0.888 & 0.721 & 0.816 & 0.783 & 0.670 & 0.688 & 0.786 & 0.758\\\hline
HAU  & 1  & 0.773 & 0.731 & 0.820 & 0.806 & 0.897 & 0.686 & 0.830 & 0.832 & 0.763 & 0.703 & 0.702 & 0.736\\\hline
LIM   & 4  & 0.756 & 0.737 & 0.760 & 0.788 & 0.886 & 0.654 & 0.808 & 0.775 & 0.756 & 0.712 & 0.701 & 0.745\\\hline
 COR  & 5  & 0.748 & 0.704 & 0.806 & 0.783 & 0.898 & 0.670 & 0.738 & 0.794 & 0.739 & 0.616 & 0.730 & 0.741\\\hline
 HYD  & 1  & 0.744 & 0.680 & 0.778 & 0.748 & 0.839 & 0.693 & 0.788 & 0.781 & 0.735 & 0.613 & 0.770 & 0.754\\\hline
 CUN  & 1  & 0.725 & 0.696 & 0.743 & 0.737 & 0.830 & 0.714 & 0.838 & 0.676 & 0.670 & 0.680 & 0.697 & 0.684\\\hline
  UNT & 3  & 0.645 & 0.667 & 0.682 & 0.635 & 0.746 & 0.558 & 0.687 & 0.676 & 0.620 & 0.539 & 0.667 & 0.609\\\hline
BOB   & 4  & 0.625 & 0.513 & 0.684 & 0.638 & 0.751 & 0.612 & 0.706 & 0.647 & 0.549 & 0.495 & 0.621 & 0.608\\\hline
KYL  & 1  & 0.590 & 0.589 & 0.603 & 0.643 & 0.634 & 0.554 & 0.663 & 0.627 & 0.569 & 0.450 & 0.649 & 0.507\\\hline
 UKP  & 2  & 0.583 & 0.592 & 0.560 & 0.624 & 0.653 & 0.558 & 0.616 & 0.631 & 0.565 & 0.456 & 0.656 & 0.489\\\hline
 MIC  & 3  & 0.430 & 0.419 & 0.386 & 0.411 & 0.519 & 0.407 & 0.488 & 0.422 & 0.384 & 0.400 & 0.500 & 0.396\\\hline
EUR   & 1  & 0.386 & 0.500 & 0.390 & 0.277 & 0.379 & 0.487 & 0.522 & 0.441 & 0.352 & 0.281 & 0.438 & 0.261\\\hline
 VTX & 5  & 0.319 & 0.367 & 0.298 & 0.179 & 0.297 & 0.159 & 0.435 & 0.340 & 0.370 & 0.201 & 0.410 & 0.230\\\hline
\end{tabular}
\caption{Results for closed task\label{tab:results:closed}}
\end{small}
\end{table*}



\begin{table*}
\begin{small}
\begin{tabular}{|p{1cm}|c|p{1cm}|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline

\multicolumn{3}{|c|}{} & \multicolumn{11}{|c|}{\bf L1 F-Score}\\\hline
{\bf Team Name} & {\bf Run} & {\bf Overall Acc.} & {\bf ARA} & {\bf CHI} & {\bf FRE} & {\bf GER} & {\bf HIN} & {\bf ITA} & {\bf JPN} & {\bf KOR} & {\bf SPA} & {\bf TEL} & {\bf TUR}\\\hline
TOR & 5  & 0.565 & 0.410 & 0.776 & 0.692 & 0.754 & 0.277 & 0.680 & 0.660 & 0.650 & 0.653 & 0.190 & 0.468\\\hline
TUE & 2  & 0.385 & 0.114 & 0.502 & 0.420 & 0.430 & 0.167 & 0.611 & 0.485 & 0.348 & 0.385 & 0.236 & 0.314\\\hline
NAI & 2  & 0.356 & 0.329 & 0.450 & 0.331 & 0.423 & 0.066 & 0.511 & 0.426 & 0.481 & 0.314 & 0.000 & 0.207\\\hline
\end{tabular}
\caption{Results for open-1 task\label{tab:results:open1}}
\end{small}
\end{table*}


\begin{table*}
\begin{small}
\begin{tabular}{|p{1cm}|c|p{1cm}|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline

\multicolumn{3}{|c|}{} & \multicolumn{11}{|c|}{\bf L1 F-Score}\\\hline
{\bf Team Name} & {\bf Run} & {\bf Overall Acc.} & {\bf ARA} & {\bf CHI} & {\bf FRE} & {\bf GER} & {\bf HIN} & {\bf ITA} & {\bf JPN} & {\bf KOR} & {\bf SPA} & {\bf TEL} & {\bf TUR}\\\hline
TUE & 1  & 0.835 & 0.798 & 0.876 & 0.844 & 0.883 & 0.777 & 0.883 & 0.836 & 0.794 & 0.846 & 0.826 & 0.818\\\hline
TOR& 4  & 0.816 & 0.770 & 0.861 & 0.840 & 0.900 & 0.704 & 0.860 & 0.834 & 0.800 & 0.816 & 0.804 & 0.790\\\hline
HYD & 1  & 0.741 & 0.677 & 0.782 & 0.755 & 0.829 & 0.693 & 0.784 & 0.777 & 0.728 & 0.613 & 0.766 & 0.744\\\hline
NAI & 3  & 0.703 & 0.676 & 0.695 & 0.708 & 0.846 & 0.618 & 0.830 & 0.677 & 0.610 & 0.663 & 0.726 & 0.688\\\hline
\end{tabular}
\caption{Results for open-2 task\label{tab:results:open1}}
\end{small}
\end{table*}


\subsection{Commonalities}


\begin{table*}[htbp]
\begin{small}
\begin{tabular}{|l|l|l|}
\hline
{\bf Feature} & {\bf Type} & {\bf Teams} \\  \hline
Word N-Grams       & 1      & CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, UAB, CYW, NAI, NRC, MIC, CAR   \\ \hline
		   & 2      & CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, COR, UAB, CYW, NAI, NRC, HAU, MIC, CAR \\ \hline
                   & 3      & UNT, MQ, JAR, KYL, CUN, COR, HAU, MIC, CAR  \\ \hline
                   & 4      & JAR, KYL, CAR   \\ \hline
		   & 5      & CAR \\ \hline	
POS N-grams        & 1      & CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, HAI, CAR  \\ \hline
		   & 2      & CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, COR, HAI, NAI, NRC, MIC, CAR   \\ \hline
                   & 3      & CN, UNT, JAR, TOR, LIM, CUN, TUE, COR, HAI, NAI, NRC, CAR     \\ \hline
                   & 4      & CN, JAR, TUE, HAI, NRC, CAR    \\ \hline
                   & 5      & TUE, CAR \\ \hline
Character N-Grams  & 1      & CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, HAI, CAR  \\ \hline
                   & 2      & CN, UNT, MQ, JAR, TOR, ITA, LIM, BOB, OSL, COR, HAI, NAI, HAU, MIC, CAR  \\ \hline
                   & 3      & CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, VTX, COR, HAI, NAI, NRC, HAU, MIC, CAR \\ \hline
                   & 4      & CN, JAR, LIM, BOB, OSL, HAI, HAU, MIC, CAR \\ \hline 
                   & 5      & CN, JAR, BOB, OSL, HAU, CAR  \\ \hline
                   & 6      & CN, JAR, OSL,   \\ \hline
	           & 7      & JAR, OSL
                   & 8-9    & JAR \\ \hline 
Function N-Grams   &        & MQ, UAB  \\ \hline
Syntactic Features & Adaptor Grammars        &   \\ \hline        
                   & TSG                 & \\ \hline
                   & Dependencies        & \\ \hline
Spelling Features  &        &   \\ \hline
\end{tabular}
\end{small}
\caption{Common Features used in Shared Task\label{tab:common-features}}
\end{table*}

Next table - Machine learning algorithms


\section{Cross Validation Results}
Upon completion of the competition, we asked the participants to
perform 10-fold cross validation on a data set consisting of the union
of TOEFL11-TRAIN an TOEFL11-DEV.  This was the same set of data
used in the first work to use any of the TOEFL11 data 
\cite{tetreault-EtAl:2012:PAPERS}, and would allow another point of 
comparison for future NLI work.  For direct comparison with 
\cite{tetreault-EtAl:2012:PAPERS}, we provided the exact folds used in 
that work.  

The results of the 10-fold cross validation is shown in Table~\ref{tab:10fold}.
Two teams had systems that performed at 84.5 or better, which is just 
slightly higher than the best team performance on the TOEFL11 data.  In
general, systems that performed well in the main competition also
performed similarly (in terms of performance and ranking) in the
cross validation experiment.


\begin{table}[htbp]
\begin{tabular}{|l|l|}
\hline
Team Name & Accuracy\\ \hline
CN & 84.55 \\ \hline
JAR & 84.5 \\ \hline
OSL & 84.1 ???\\ \hline
BUC & 82.6 \\ \hline
TUE & 82.4 \\ \hline
CAR & 82.2 \\ \hline
NAI & 82.09 \\ \hline
\cite{tetreault-EtAl:2012:PAPERS} &  80.9 \\ \hline
HAU & 79.9 \\ \hline
LIM & 75.89  \\ \hline
CUN & 74.2\\ \hline
UNT  & 63.77 \\ \hline
MIC & 63 \\ \hline
MQ  & ?? \\ \hline

\end{tabular}
\caption{Results for 10-fold cross-validation on TOEFL-Train $+$ TOEFL-Dev \label{tab:10fold}}
\end{table}

\section{Future Work} -are we doing this again next year?  what would we do differently?


\section*{Acknowledgments}

Thank the following people:  Derrick Higgins and TOEFL crowd for
making the TOEFL11 possible.  Patrick Houghton for xyz...all the people
who participated in the shared task.  BEA organizers...



\bibliographystyle{naaclhlt2013}
\bibliography{nli-relatedwork}


\end{document}
