%
% File naaclhlt2012.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{linguex}
\usepackage{subfig}
\usepackage[table]{xcolor}    % also loads colortbl
\usepackage[T1]{fontenc}

\setlength\titlebox{6.5cm}    % Expanding the titlebox



\title{A Report on the First Native Language Identification Shared Task}

\author{Joel Tetreault$^{*}$, Daniel Blanchard$^{\dag}$ and Aoife Cahill$^{\dag}$\\
\\
  { $^{*}$ Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USA}\\
  { {\tt Joel.Tetreault@nuance.com}} \\
  { $^{\dag}$ Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA}\\
  { {\tt \{dblanchard, acahill\}@ets.org}}\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Native Language Identification, or NLI, is the task of automatically classifying
the L1 of a writer based solely on his or her essay.  This problem area has
seen a spike in interest in recent years as it can have an impact on
educational applications tailored towards non-native speakers of a language,
as well as authorship profiling.  
While there has been a growing body of work in NLI, it has been difficult to compare 
approaches because of the different approaches to pre-processing the data, different 
sub-sets of languages evaluated and different splits of the data used.
In this shared task, the first
ever for Native Language Identification, we sought to
address the above issues by providing a large corpus designed specifically for NLI,
in addition to providing an environment for systems to be directly compared.
In this paper, we report the results of the shared task.
A total of 29 teams from around the world competed across
three different subtasks.
\end{abstract}

\section{Introduction}
\label{sec-intro}
One quickly growing subfield in NLP is the task of identifying the native
language (L1) of a writer based solely on a sample of their writing. The task
is framed as a classification problem where the set of L1s is known \emph{a priori}.
Most work has focused on identifying the native language of writers
learning English as a second language. To date this topic has motivated
several papers and research projects.

Native Language Identification (NLI) can be useful for a number of
applications. NLI can be used in educational settings to provide more
targeted feedback to language learners about their errors. It is
well known that speakers of different languages make different kinds of
errors when learning a language \cite{SwanSmith01}. A writing tutor system
which can detect the native language of the learner will be able to tailor
the feedback about the error and contrast it with common
properties of the learner's language. In addition, native language is
often used as a feature that goes into authorship profiling
\cite{estival2007author}, which is frequently used in forensic linguistics.

Despite the growing interest in this field, development has been encumbered
by two issues. First is the issue of data. Evaluating an NLI system requires
 a corpus containing texts in a language other than the native language of
the writer. Because of a scarcity of such corpora, most work has used
the International Corpus of Learner English (ICLEv2) \cite{Granger2009}
for training and evaluation since it contains several hundred essays
written by college-level English language learners. However,
this corpus is quite small for training and testing statistical systems
which makes it difficult to tell whether the systems that are developed can
scale well to larger data sets or to different domains.

Since the ICLE corpus was not designed with the task of NLI in mind, the
usability of the corpus for this task is further compromised by idiosyncrasies
in the data such as topic bias (as shown by \newcite{brooke2011native}) and the
occurrence of characters which only appear in essays written by speakers of
certain languages \cite{tetreault-EtAl:2012:PAPERS}. As a result, it is hard to
draw conclusions about which features actually perform best. The second issue is
that there has been little consistency in the field in the use of
cross-validation, the number of L1s, and which L1s are used. As a result,
comparing one approach to another has been extremely difficult.

The first shared task in Native Language Identification (NLI) is intended to
better unify this community and help the field progress. The Shared Task
addresses the two deficiencies above by first using a new corpus (\textsc{toef11},
discussed in Section~\ref{sec-data}) which is larger than the ICLE and designed specifically for the
task of NLI and second, by providing a common set of L1s and evaluation
standards that everyone will use for this competition, thus facilitating direct
comparison of approaches. In this report we describe the methods most
participants used, the data they evaluated their systems on, the three sub-tasks
involved, the results achieved by the different teams, and some suggestions
ideas about what we can do for the next iteration of the NLI shared task.

In the following section, we provide a summary of the prior work in
Native Language Identification.  Next, in Section~\ref{sec-data} we describe 
the \textsc{toefl11} corpus used for training, development and testing in 
this shared task. In Sections~\ref{sec-teams}....

\section{Related Work}
\label{sec-relatedwork}
In this section, we provide an overview of some of the common
approaches used for NLI prior to this shared task.  While a
comprehensive review is outside the scope of this paper, we have compiled a 
bibliography of related work in the field. It can be downloaded 
from the NLI Shared Task website.\footnote{http://nlisharedtask2013.org/bibliography-of-related-work-in-nli}.

To date, nearly all approaches have treated the task of NLI as a 
supervised classification problem where statistical models are trained on 
data from the different L1s.  The work of \shortcite{koppel2005determining}
was the first in the field and they explored a multitude of features, many
of which are employed in several of the systems in the shared tasks.  These
features included character and POS n-grams, content and function words,
as well as spelling and grammatical errors (since language learners have
tendencies to make certain errors based on their L1 \cite{SwanSmith01}).
An SVM model was trained on these features extracted from a subsection of 
ICLE corpus consisting of 5 L1s.   These features would figure prominently
in later work such as \cite{wong-dras:2011:EMNLP} and \cite{tetreault-EtAl:2012:PAPERS}.  

N-gram features (word, character and POS)  have figured 
prominently prior work.  Not only are they
easy to compute, but they can be quite predictive.  However, there are
many variations on the features.  Past reseach efforts have explored
different n-gram windows (though most tend to focus on uni-grams and bi-grams),different thresholds for how many n-grams
to include as well as whether to encode the feature as binary (presence
or absence of the particular n-gram) or as a normalized count.  

The inclusion of syntactic features have been a focus in recent work.
Wong and Dras \shortcite{wong-dras:2011:EMNLP} explored the use of production rules
from two parsers and Swanson and Charniak \shortcite{swanson-charniak:2012:ACL2012short} explored
the use of Tree Substitution Grammars (TSGs).  Tetreault et al. \shortcite{tetreault-EtAl:2012:PAPERS} also investigated the use of TSGs as well as dependency
features extracted from the Stanford parser.  

Other approaches to NLI have included the use of Latent Dirichlet Analysis
to cluster features \cite{Sze-MengJojoWongMarkDrasMarkJohnson:2011:ALTA2011},
adaptor grammars \cite{wong-dras-johnson:2012:EMNLP-CoNLL}, and 
language models \cite{tetreault-EtAl:2012:PAPERS}.  Additionally, there has
been research into the effects of training and testing on different 
corpora \cite{brooke2011native}.  

Much of the aforementioned work takes the perspective of optimizing for the 
task of Native Language Identification, that is, what is the best way of 
modeling the problem to get the highest system accuracy?  The problem 
of Native Language Identification is also of interest to researchers in
Second Language Acquisition where they seek to explain syntactic transfer
in learner language \cite{jarvis2012approaching}.

\section{Data}
\label{sec-data}
The dataset for the task was the new \textsc{toefl11} corpus
\cite{blanchard-tetreault-higgins-cahill-chodorow:2013:TOEFL11-RR}. \textsc{toefl11}
consists of essays written during a high-stakes college-entrance test, the Test
of English as a Foreign Language (TOEFL\textsuperscript{\textregistered}). The
corpus contains 1,100 essays per language sampled as evenly as possible from 8
prompts (i.e., topics) along with score levels (low/medium/high) for each essay.
The 11 native languages covered by our corpus are: Arabic (ARA), Chinese
(CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese
(JAP), Korean (KOR), Spanish (SPA), Telugu (TEL), and Turkish (TUR).

The \textsc{toefl11} corpus was designed specifically to support the task of native
language identification.  Because all of the essays were collected through ETS's
operational test delivery system for the TOEFL\textsuperscript{\textregistered}
test, the encoding and storage of all texts in the corpus is consistent.
Furthermore, the sampling of essays was designed to ensure approximately equal
representation of native languages across topics, insofar as this was possible.

For the shared task, the corpus was split into three sets: training (\textsc{toefl11-train}),
development (\textsc{toefl11-dev}), and test (\textsc{toefl11-test}). The train corpus consisted
of 900 essays per L1, the development set consisted of 100 essays per L1,
and the test set consisted of another 100 essays per L1.
Although the overall \textsc{toefl11} corpus was sampled as evenly
as possible with regard to language and prompts, the distribution for each
language is not exactly the same in the training, development and test sets (see
Tables~\ref{tab:prompts-train}, \ref{tab:prompts-dev}, and
\ref{tab:prompts-test}). In fact, the distribution is much closer between the
training and test sets, as there are several languages for which there are no
essays for a given prompt in the development set, whereas there are none in the
training set, and only one, Italian, for the test set.

It should be noted that in the first instantiation of the corpus,
presented in Tetreault et al. \shortcite{tetreault-EtAl:2012:PAPERS}, we used \textsc{toefl11}
to denote the body of data consisting of \textsc{toefl11-train} and \textsc{toefl11-dev}. 
However, in this shared task, we added 1,100 sentences for a test 
set and thus use the term \textsc{toefl11} to now denote the corpus 
consisting of the \textsc{train}, \textsc{dev} and \textsc{test} sets.  We expect the corpus
to be released through the the Linguistic Data Consortium in 2013.


\begin{table*}[ht]
\begin{small}
\begin{center}

\subfloat[Training Set]{\label{tab:prompts-train}
        \begin{tabular}{lrrrrrrrr}
            \hline
            \textbf{Lang.} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8}\\ \hline
            ARA     &  113 & 113 & 113 & 112 & 112 & 113 & 112 & 112 \\ \hline
            CHI    &  113 & 113 & 113 & 112 & 112 & 113 & 112 & 112 \\ \hline
            FRE     &  128 & 128 & 76  & 127 & 127 & 60  & 127 & 127 \\ \hline
            GER     &  125 & 125 & 125 & 125 & 125 & 26  & 125 & 124 \\ \hline
            HIN      &  132 & 132 & 132 & 71  & 132 & 38  & 132 & 131 \\ \hline
            ITA    &  142 & 70  & 122 & 141 & 141 & 12  & 141 & 131 \\ \hline
            JAP   &  108 & 114 & 113 & 113 & 113 & 113 & 113 & 113 \\ \hline
            KOR     &  113 & 113 & 113 & 112 & 112 & 113 & 112 & 112 \\ \hline
            SPA    &  124 & 120 & 38  & 124 & 123 & 124 & 124 & 123 \\ \hline
            TEL     &  139 & 139 & 139 & 41  & 139 & 26  & 139 & 138 \\ \hline
            TUR    &  132 & 132 & 72  & 132 & 132 & 37  & 132 & 131 \\ \hline \hline
            Total      &  1369 & 1299 & 1156 & 1210 & 1368 & 775 & 1369 & 1354 \\ \hline
        \end{tabular}
}
~\\~\\
\subfloat[Dev Set]{\label{tab:prompts-dev}
        \begin{tabular}{lrrrrrrrr}
            \hline
            \textbf{Lang.} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8}\\ \hline
            ARA     &   12 & 13 & 13 & 13 & 14 & 7  & 14 & 14  \\ \hline
            CHI    &   14 & 14 & 0  & 15 & 15 & 14 & 13 & 15  \\ \hline
            FRE     &   17 & 18 & 0  & 14 & 19 & 0  & 13 & 19  \\ \hline
            GER     &   15 & 15 & 16 & 10 & 13 & 0  & 15 & 16  \\ \hline
            HIN      &   16 & 17 & 17 & 0  & 17 & 0  & 16 & 17  \\ \hline
            ITA    &   18 & 0  & 0  & 30 & 31 & 0  & 21 & 0   \\ \hline
            JAP   &   0  & 14 & 15 & 14 & 15 & 14 & 14 & 14  \\ \hline
            KOR     &   15 & 8  & 15 & 2  & 13 & 15 & 16 & 16  \\ \hline
            SPA    &   7  & 0  & 0  & 21 & 7  & 21 & 21 & 23  \\ \hline
            TEL     &   16 & 17 & 17 & 0  & 17 & 0  & 16 & 17  \\ \hline
            TUR    &   22 & 4  & 0  & 22 & 7  & 0  & 22 & 23  \\ \hline \hline
            Total      &   152& 120& 93 & 141& 168& 71 & 181& 174 \\ \hline
        \end{tabular}
}
~\\~\\
\subfloat[Test Set]{\label{tab:prompts-test}
        \begin{tabular}{lrrrrrrrr}
            \hline
            \textbf{Lang.} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8}\\ \hline
            ARA     &   13 & 11 & 12 & 14 & 10 & 13 & 12 & 15 \\ \hline
            CHI    &   13 & 14 & 13 & 13 & 7  & 14 & 14 & 12 \\ \hline
            FRE     &   13 & 14 & 11 & 15 & 14 & 8  & 11 & 14 \\ \hline
            GER     &   15 & 14 & 16 & 16 & 12 & 2  & 12 & 13 \\ \hline
            HIN      &   13 & 13 & 14 & 15 & 7  & 15 & 10 & 13 \\ \hline
            ITA    &   13 & 19 & 16 & 16 & 15 & 0  & 11 & 10 \\ \hline
            JAP   &   8  & 14 & 12 & 11 & 10 & 15 & 14 & 16 \\ \hline
            KOR     &   12 & 12 & 8  & 14 & 12 & 14 & 13 & 15 \\ \hline
            SPA    &   10 & 13 & 16 & 14 & 4  & 12 & 15 & 16 \\ \hline
            TEL     &   10 & 10 & 11 & 14 & 13 & 15 & 11 & 16 \\ \hline
            TUR    &   15 & 9  & 18 & 16 & 8  & 6  & 13 & 15 \\ \hline \hline
            Total      &   135&143 & 147& 158& 112& 114& 136& 155 \\ \hline
        \end{tabular}
}
\end{center}
\end{small}
\caption{Number of essays per language per prompt in each data set}
\end{table*}


\section{NLI Shared Task Description}

The shared task consisted of three sub-tasks. For each task, the
test set was \textsc{toefl11-TEST} and only the type of training data varied from
task to task.

\begin{itemize}
\item {\bf Closed-Training}:  The first and main task was the 11 way
classification task using only the \textsc{toefl11-train} and \textsc{toefl11-dev} for training.
\item {\bf Open-Training-1}: The second task allowed the use of any amount or
type of training data (as is done by \newcite{brooke2011native})
\emph{excluding} any data from the \textsc{toefl11}, but still evaluated on \textsc{toefl11-test}.
\item {\bf Open-Training-2}: The third task allowed the use of \textsc{toefl11-train} and
\textsc{toefl11-dev} combined with any other additional data. This most closely reflects
a real-world scenario.
\end{itemize}

Additionally, each team could submit up to 5 different
systems per task.  This allows a team to experiment with different
variations of their core system.

The training data was released on January 14, with the development
data and evaluation script released nearly a month later on February 12.
The train and dev data contained an index file with the L1 for each
essay in those sets.  The previously unseen and unlabeled test data was
released on March 11 and teams had 8 days to submit their system predictions.
The predictions for each system
were encoded in a CSV file, where each line contained the file ID of a
file in TOEFL-TEST and the corresponding L1 prediction made by the
system.  Each CSV file was emailed to the NLI organizers and then
evaluated against the gold standard.
% Since this was a ``blind'' competition, the L1s for each of the essays in the test set were not released until after the submission deadline.


\section{Teams}
\label{sec-teams}
In total, 29 teams competed in the shared task competition, with 24 teams
electing to write papers describing their system(s).  The list
of participating times, along with their team abbreviations, can be
found in Table~\ref{tab:teams}.


\begin{table*}[htbp]
\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Team Name} & \textbf{Team Abbreviation} \\ \hline
Bobicev & BOB  \\ \hline
Chonger & CHO  \\ \hline
CMU-Haifa & HAI  \\ \hline
Cologne-Nijmegen &  CN \\ \hline
CoRAL Lab @ UAB &  COR \\ \hline
CUNI (Charles University) &  CUN \\ \hline
cywu &  CYW \\ \hline
dartmouth & DAR  \\ \hline
eurac & EUR  \\ \hline
HAUTCS & HAU  \\ \hline
ItaliaNLP &  ITA \\ \hline
Jarvis &   JAR\\ \hline
kyle, crossley, dai, mcnamara & KYL  \\ \hline
LIMSI & LIM  \\ \hline
LTRC IIIT Hyderabad &  HYD \\ \hline
Michigan &  MIC \\ \hline
MITRE ``Carnie'' & CAR  \\ \hline
MQ &   MQ\\ \hline
NAIST &  NAI \\ \hline
NRC &  NRC \\ \hline
Oslo NLI & OSL  \\ \hline
Toronto &  TOR \\ \hline
Tuebingen &   TUE\\ \hline
Ualberta &  UAB \\ \hline
UKP &  UKP \\ \hline
Unibuc &  BUC \\ \hline
UNT &   UNT\\ \hline
UTD &  UTD \\ \hline
VTEX &  VTX \\ \hline

\end{tabular}
\end{center}
\caption{Participating Teams and Team Abbreviations\label{tab:teams}}
\end{table*}

\section{Shared Task Results}
\label{sec-results}
This section summarizes the results of the shared task.  For each subtask,
we have tables listing the top submission of each team, its performance in
accuracy, and its performance by L1.\footnote{For those interested seeing
the results of all submissions, please contact the authors.}

Table \ref{tab:results:closed} shows results for the Closed sub-task where teams
developed systems which were trained solely on \textsc{toefl11-train} and \textsc{toefl11-dev}.  This
was the most popular subtask with 29 teams competing with 116 submissions total
for the subtask.  Most teams opted to submit 4 or 5 runs.

\begin{table*}
\begin{footnotesize}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{|p{1cm}|c|p{1cm}|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\rowcolor{gray!50}
\multicolumn{3}{|c|}{} & \multicolumn{11}{c|}{\bf L1 F-Score}\\\hline
{\bf Team Name} & {\bf Run} & {\bf Overall Acc.} & {\bf ARA} & {\bf CHI} & {\bf FRE} & {\bf GER} & {\bf HIN} & {\bf ITA} & {\bf JPN} & {\bf KOR} & {\bf SPA} & {\bf TEL} & {\bf TUR}\\\hline
  JAR & 2  & 0.836 & 0.785 & 0.856 & 0.860 & 0.893 & 0.775 & 0.905 & 0.854 & 0.813 & 0.798 & 0.802 & 0.854\\\hline
OSL   & 2  & 0.834 & 0.816 & 0.850 & 0.874 & 0.912 & 0.792 & 0.873 & 0.828 & 0.806 & 0.783 & 0.792 & 0.840\\\hline
 BUC  & 5  & 0.827 & 0.840 & 0.866 & 0.853 & 0.931 & 0.736 & 0.873 & 0.851 & 0.812 & 0.779 & 0.760 & 0.796\\\hline
CAR   & 2  & 0.826 & 0.859 & 0.847 & 0.810 & 0.921 & 0.762 & 0.877 & 0.825 & 0.827 & 0.768 & 0.802 & 0.790\\\hline
  TUE & 1  & 0.822 & 0.810 & 0.853 & 0.806 & 0.897 & 0.768 & 0.883 & 0.842 & 0.776 & 0.772 & 0.824 & 0.812\\\hline
 NRC  & 4  & 0.818 & 0.804 & 0.845 & 0.848 & 0.916 & 0.745 & 0.903 & 0.818 & 0.790 & 0.788 & 0.755 & 0.790\\\hline
HAI   & 1  & 0.815 & 0.804 & 0.842 & 0.835 & 0.903 & 0.759 & 0.845 & 0.825 & 0.806 & 0.776 & 0.789 & 0.784\\\hline
 CN  & 2  & 0.814 & 0.778 & 0.845 & 0.848 & 0.882 & 0.744 & 0.857 & 0.812 & 0.779 & 0.787 & 0.784 & 0.827\\\hline
 NAI  & 1  & 0.811 & 0.814 & 0.829 & 0.828 & 0.876 & 0.755 & 0.864 & 0.806 & 0.789 & 0.757 & 0.793 & 0.802\\\hline
 UTD  & 2  & 0.809 & 0.778 & 0.846 & 0.832 & 0.892 & 0.731 & 0.866 & 0.846 & 0.819 & 0.715 & 0.784 & 0.784\\\hline
 UAB  & 3  & 0.803 & 0.820 & 0.804 & 0.822 & 0.905 & 0.724 & 0.850 & 0.811 & 0.736 & 0.777 & 0.792 & 0.786\\\hline
 TOR  & 1  & 0.802 & 0.754 & 0.827 & 0.827 & 0.878 & 0.722 & 0.850 & 0.820 & 0.808 & 0.747 & 0.784 & 0.798\\\hline
  MQ & 4  & 0.801 & 0.800 & 0.828 & 0.789 & 0.885 & 0.738 & 0.863 & 0.826 & 0.780 & 0.703 & 0.782 & 0.802\\\hline
 CYW  & 1  & 0.797 & 0.769 & 0.839 & 0.782 & 0.833 & 0.755 & 0.842 & 0.815 & 0.770 & 0.741 & 0.828 & 0.788\\\hline
DAR   & 2  & 0.781 & 0.761 & 0.806 & 0.812 & 0.870 & 0.706 & 0.846 & 0.788 & 0.776 & 0.730 & 0.723 & 0.767\\\hline
 ITA  & 1  & 0.779 & 0.738 & 0.775 & 0.832 & 0.873 & 0.711 & 0.860 & 0.788 & 0.742 & 0.708 & 0.762 & 0.780\\\hline
CHO  & 1  & 0.775 & 0.764 & 0.835 & 0.798 & 0.888 & 0.721 & 0.816 & 0.783 & 0.670 & 0.688 & 0.786 & 0.758\\\hline
HAU  & 1  & 0.773 & 0.731 & 0.820 & 0.806 & 0.897 & 0.686 & 0.830 & 0.832 & 0.763 & 0.703 & 0.702 & 0.736\\\hline
LIM   & 4  & 0.756 & 0.737 & 0.760 & 0.788 & 0.886 & 0.654 & 0.808 & 0.775 & 0.756 & 0.712 & 0.701 & 0.745\\\hline
 COR  & 5  & 0.748 & 0.704 & 0.806 & 0.783 & 0.898 & 0.670 & 0.738 & 0.794 & 0.739 & 0.616 & 0.730 & 0.741\\\hline
 HYD  & 1  & 0.744 & 0.680 & 0.778 & 0.748 & 0.839 & 0.693 & 0.788 & 0.781 & 0.735 & 0.613 & 0.770 & 0.754\\\hline
 CUN  & 1  & 0.725 & 0.696 & 0.743 & 0.737 & 0.830 & 0.714 & 0.838 & 0.676 & 0.670 & 0.680 & 0.697 & 0.684\\\hline
  UNT & 3  & 0.645 & 0.667 & 0.682 & 0.635 & 0.746 & 0.558 & 0.687 & 0.676 & 0.620 & 0.539 & 0.667 & 0.609\\\hline
BOB   & 4  & 0.625 & 0.513 & 0.684 & 0.638 & 0.751 & 0.612 & 0.706 & 0.647 & 0.549 & 0.495 & 0.621 & 0.608\\\hline
KYL  & 1  & 0.590 & 0.589 & 0.603 & 0.643 & 0.634 & 0.554 & 0.663 & 0.627 & 0.569 & 0.450 & 0.649 & 0.507\\\hline
 UKP  & 2  & 0.583 & 0.592 & 0.560 & 0.624 & 0.653 & 0.558 & 0.616 & 0.631 & 0.565 & 0.456 & 0.656 & 0.489\\\hline
 MIC  & 3  & 0.430 & 0.419 & 0.386 & 0.411 & 0.519 & 0.407 & 0.488 & 0.422 & 0.384 & 0.400 & 0.500 & 0.396\\\hline
EUR   & 1  & 0.386 & 0.500 & 0.390 & 0.277 & 0.379 & 0.487 & 0.522 & 0.441 & 0.352 & 0.281 & 0.438 & 0.261\\\hline
 VTX & 5  & 0.319 & 0.367 & 0.298 & 0.179 & 0.297 & 0.159 & 0.435 & 0.340 & 0.370 & 0.201 & 0.410 & 0.230\\\hline
\end{tabular}
\caption{Results for closed task\label{tab:results:closed}}
\end{footnotesize}
\end{table*}

The Open subtasks had far fewer submissions.  Table \ref{tab:results:open1} shows
results for the Open-1 sub-task where teams could train systems using any training
data {\it except} for \textsc{toefl11-train} and \textsc{toefl11-dev}.  Three teams competed in this
subtask for a total of 13 submissions.

Table \ref{tab:results:open2} shows the results for the third subtask ``Open-2''.  Four
teams competed in this task for a total of 15 submissions.  Interestingly for both
of the Open subtasks, despite having access to different (and possibly more) data, the
top system from the Closed sub-task was slightly higher than the top system from the
Open-2 subtask (0.836 to 0.835).  One way of viewing this result is that it is
preferable to keep the training data as close in composition to the test data.  JRT:  not sure
if I want to make this claim.

{\bf JOEL: say what data they trained on!!!}

\begin{table*}
\begin{footnotesize}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{|p{1cm}|c|p{1cm}|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\rowcolor{gray!50}
\multicolumn{3}{|c|}{} & \multicolumn{11}{c|}{\bf L1 F-Score}\\\hline
{\bf Team Name} & {\bf Run} & {\bf Overall Acc.} & {\bf ARA} & {\bf CHI} & {\bf FRE} & {\bf GER} & {\bf HIN} & {\bf ITA} & {\bf JPN} & {\bf KOR} & {\bf SPA} & {\bf TEL} & {\bf TUR}\\\hline
TOR & 5  & 0.565 & 0.410 & 0.776 & 0.692 & 0.754 & 0.277 & 0.680 & 0.660 & 0.650 & 0.653 & 0.190 & 0.468\\\hline
TUE & 2  & 0.385 & 0.114 & 0.502 & 0.420 & 0.430 & 0.167 & 0.611 & 0.485 & 0.348 & 0.385 & 0.236 & 0.314\\\hline
NAI & 2  & 0.356 & 0.329 & 0.450 & 0.331 & 0.423 & 0.066 & 0.511 & 0.426 & 0.481 & 0.314 & 0.000 & 0.207\\\hline
\end{tabular}
\caption{Results for open-1 task\label{tab:results:open1}}
\end{footnotesize}
\end{table*}


\begin{table*}
\begin{footnotesize}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{|p{1cm}|c|p{1cm}|r|r|r|r|r|r|r|r|r|r|r|r|}
\hline
\rowcolor{gray!50}
\multicolumn{3}{|c|}{} & \multicolumn{11}{c|}{\bf L1 F-Score}\\\hline
{\bf Team Name} & {\bf Run} & {\bf Overall Acc.} & {\bf ARA} & {\bf CHI} & {\bf FRE} & {\bf GER} & {\bf HIN} & {\bf ITA} & {\bf JPN} & {\bf KOR} & {\bf SPA} & {\bf TEL} & {\bf TUR}\\\hline
TUE & 1  & 0.835 & 0.798 & 0.876 & 0.844 & 0.883 & 0.777 & 0.883 & 0.836 & 0.794 & 0.846 & 0.826 & 0.818\\\hline
TOR& 4  & 0.816 & 0.770 & 0.861 & 0.840 & 0.900 & 0.704 & 0.860 & 0.834 & 0.800 & 0.816 & 0.804 & 0.790\\\hline
HYD & 1  & 0.741 & 0.677 & 0.782 & 0.755 & 0.829 & 0.693 & 0.784 & 0.777 & 0.728 & 0.613 & 0.766 & 0.744\\\hline
NAI & 3  & 0.703 & 0.676 & 0.695 & 0.708 & 0.846 & 0.618 & 0.830 & 0.677 & 0.610 & 0.663 & 0.726 & 0.688\\\hline
\end{tabular}
\caption{Results for open-2 task\label{tab:results:open2}}
\end{footnotesize}
\end{table*}







\section{Cross Validation Results}
\
Upon completion of the competition, we asked the participants to
perform 10-fold cross validation on a data set consisting of the union
of \textsc{toefl11-train} and \textsc{toefl11-dev}.  This was the same set of data
used in the first work to use any of the \textsc{toefl11} data
\cite{tetreault-EtAl:2012:PAPERS}, and would allow another point of
comparison for future NLI work.  For direct comparison with
\newcite{tetreault-EtAl:2012:PAPERS}, we provided the exact folds used in
that work.

The results of the 10-fold cross validation are shown in Table~\ref{tab:10fold}.
Two teams had systems that performed at 84.5 or better, which is just
slightly higher than the best team performance on the \textsc{toefl11-TEST} data.  In
general, systems that performed well in the main competition also
performed similarly (in terms of performance and ranking) in the
cross validation experiment.  Please note that we report results as they
are reported ``as is'' in the respective papers, hence the differences in
significant digits.




\begin{table}[htbp]
\begin{tabular}{|l|l|}
\hline
\textbf{Team Name} & \textbf{Accuracy}\\ \hline
CN & 84.55 \\ \hline
JAR & 84.5 \\ \hline
OSL & 83.9 \\ \hline
BUC & 82.6 \\ \hline
MQ  & 82.46 \\ \hline
TUE & 82.4 \\ \hline
CAR & 82.2 \\ \hline
NAI & 82.09 \\ \hline
\newcite{tetreault-EtAl:2012:PAPERS} &  80.9 \\ \hline
HAU & 79.9 \\ \hline
LIM & 75.89  \\ \hline
CUN & 74.2\\ \hline
UNT  & 63.77 \\ \hline
MIC & 63 \\ \hline


\end{tabular}
\caption{Results for 10-fold cross-validation on TOEFL-Train $+$ TOEFL-Dev
\label{tab:10fold}}
\end{table}

\section{Discussion of Approaches}
\label{sec-commonalities}
With so many teams competing in the shared task competition, we investigated
whether there were any commonalities in learning methods or features 
between the teams.  In this section, we provide a coarse grained summary
of the common machine learning methods teams employed as well as some of 
the common features.  Our summary is based on the information provided
in the 24 team reports.\footnote{29 teams competed but 24 teams submitted reports}.  

\begin{table*}[htbp]
\begin{small}
\begin{tabular}{|l|p{9.5cm}|}
\hline
{\bf Machine Learning} & {\bf Teams} \\  \hline
SVM                    & CN, UNT, MQ, JAR, TOR, ITA, CUN, TUE, COR, NRC, HAU, MIC, CAR  \\ \hline
MaxEnt / logistic regression                 & LIM, HAI, CAR  \\ \hline
Ensemble               & MQ, ITA, NRC, CAR           \\ \hline
Discriminant Function Analysis & KYL \\ \hline
String Kernels / LRD   & BUC          \\ \hline
PPM                    & BOB \\ \hline
k-NN                   & VTX \\ \hline
\end{tabular}
\end{small}
\caption{Machine Learning algorithms used in Shared Task\label{tab:common-machine-learning}}
\end{table*}


\begin{table*}[htbp]
\begin{small}
\begin{tabular}{|l|l|p{9.5cm}|}
\hline
{\bf Feature} & {\bf Type} & {\bf Teams} \\  \hline
Word N-Grams       & 1      & CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, UAB, CYW, NAI, NRC, MIC, CAR   \\ \hline
		   & 2      & CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, COR, UAB, CYW, NAI, NRC, HAU, MIC, CAR \\ \hline
                   & 3      & UNT, MQ, JAR, KYL, CUN, COR, HAU, MIC, CAR  \\ \hline
                   & 4      & JAR, KYL, CAR   \\ \hline
		   & 5      & CAR \\ \hline
POS N-grams        & 1      & CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, HAI, CAR  \\ \hline
		   & 2      & CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, COR, HAI, NAI, NRC, MIC, CAR   \\ \hline
                   & 3      & CN, UNT, JAR, TOR, LIM, CUN, TUE, COR, HAI, NAI, NRC, CAR     \\ \hline
                   & 4      & CN, JAR, TUE, HAI, NRC, CAR    \\ \hline
                   & 5      & TUE, CAR \\ \hline
Character N-Grams  & 1      & CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, HAI, CAR  \\ \hline
                   & 2      & CN, UNT, MQ, JAR, TOR, ITA, LIM, BOB, OSL, COR, HAI, NAI, HAU, MIC, CAR  \\ \hline
                   & 3      & CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, VTX, COR, HAI, NAI, NRC, HAU, MIC, CAR \\ \hline
                   & 4      & CN, JAR, LIM, BOB, OSL, HAI, HAU, MIC, CAR \\ \hline
                   & 5      & CN, JAR, BOB, OSL, HAU, CAR  \\ \hline
                   & 6      & CN, JAR, OSL,   \\ \hline
	           & 7      & JAR, OSL \\ \hline
                   & 8-9    & JAR \\ \hline
Function N-Grams   &        & MQ, UAB  \\ \hline
Syntactic Features & Dependencies        & MQ, TOR, ITA, TUE, NAI, NRC \\ \hline
                   & TSG                 & MQ, TOR, NAI,  \\ \hline
                   & CF Productions      & TOR, \\ \hline
		   & Adaptor Grammars  & MQ  \\ \hline
Spelling Features  &        &   LIM,CN  \\ \hline
\end{tabular}
\end{small}
\caption{Common Features used in Shared Task\label{tab:common-features}}
\end{table*}


\section{Summary} 
We consider the first edition of the shared task a success as we had
29 teams competing, which we consider a large number for any
shared task.  We were also delighted to see many teams build on
prior work but also try novel approaches.  It is our hope that 
finally having an evaluation on a common data set with set
evaluation standards will allow researchers to learn from each
other on what works well and what does not, and thus the
field can progress more rapidly.  The evaluation scripts are 
publicly available and the data will become 
available through the Linguistic Data Consortium in 2013.  

For future editions of the NLI shared task, we think it would be
interesting to expand the scope of NLI from identifying the L1
of student essays to be able to identify the L1 of any piece of 
writing.  The ICLE and \textsc{TOEFL11} corpora are both
collections of academic writing and thus it may be the case that
certain features or methodologies generalize better to other
writing genres and domains.  For those interested in robust
NLI approaches, please refer to the TOR team shared task report
as well as \cite{brooke-hirst:2012:PAPERS}.

In addition, it would be interesting to include evaluation by 
proficiency level
as language learners make different types of errors
and may even have stylistic differences in their writing as their
proficiency progresses.  

Another related work - how well do human raters do?


\section*{Acknowledgments}
We would like to thank Derrick Higgins and members of Educational
Testing Service for assisting us in making the \textsc{TOEFL11} essays
available for this shared task.  We would also like to thank
Patrick Houghton for assisting in the evaluation phase of the shared task.
In addition, thanks goes to the BEA8 Organizers (Joel Tetreault,
Jill Burstein and Claudia Leacock) for hosting the shared
task with their workshop.  Finally, we would like to thank all the teams for 
participating in this first shared task and making it a success.
Their feedback, patience and excitement levels made organizing this
shared task a great experience.





\bibliographystyle{naaclhlt2013}
\bibliography{nli-relatedwork}


\end{document}
